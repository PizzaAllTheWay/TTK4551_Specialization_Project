\subsection{GTSAM}
Georgia Tech Smoothing And Mapping (GTSAM) is a BSD-licensed C++ library that lets us model estimation problems as factor graphs and solve them efficiently with both batch optimizers and the incremental iSAM2/Bayes tree methods. The guiding idea is to express what we know as a product of small factors, each touching only a few variables, and then exploit sparsity and variable elimination to compute fast, stable MAP estimates. In practice this gives a single, uniform toolkit for SLAM (2D/3D), visual odometry/SLAM, structure from motion, calibration, and related inference tasks, all built on the same mathematical foundation described in the iSAM2 and Bayes tree papers\cite{GTSAM_handbook,iSAM2_paper,Bayes_tree_for_SLAM_paper}.

\paragraph{Design philosophy: graph first, values separate.}
GTSAM cleanly separates the \textit{``model''} from the \textit{``state''}. A \texttt{NonlinearFactorGraph} stores our factors (priors, odometry, landmark/vision measurements, loop closures), while a \texttt{Values} container holds one current assignment to the unknowns (poses, landmarks, intrinsics, etc...). We can change the estimate without touching the graph, and we can add/remove factors without invalidating unrelated variables. This mirrors the math $f(\Theta)=\prod_i f_i(\Theta_i)$. The graph captures structure and sparsity. A particular $\Theta$ lets us evaluate or optimize. \cite{GTSAM_handbook}

\paragraph{Core building blocks.}
Variables use compact \textit{``keys''} (eks: \texttt{Symbol('x',i)} for pose $x_i$, \texttt{Symbol('l',j)} for landmark $l_j$). You build the problem by adding small, typed \textit{``factors''}, each encoding one piece of sensor information plus its noise model. For example \texttt{PriorFactor<Pose2>}, \texttt{BetweenFactor<Pose2>}, \texttt{BearingRangeFactor2D}, camera factors like \texttt{GenericProjectionFactor<Cal3\_S2>}, and many others. Noise models are explicit and first class (\texttt{noiseModel::Isotropic}, \texttt{noiseModel::Diagonal}, robust M–estimators), so units and weighting are clear and consistent. There are different ways to represent rotations in 3D space, however GTSAM has defined poses in Lie groups, this is a mathematical way to describe 3D space including rotation in an easy and intuitive to handle way. Because poses live on curved rotation/pose spaces (SE(2)/SE(3)), GTSAM updates them using \textit{``local coordinates''} and a \textit{``retraction''} operator. GTSAM computes a small 3D/6D increment in a flat tangent space and then maps it back to a valid pose, avoiding angle wrap around and keeping rotations proper. In practice, Gauss–Newton/Levenberg–Marquardt linearization methods ``just works'' with orientations, no ad hoc hacks needed. The end result is that writing SLAM code feels like drawing the factor graph, add one factor per measurement between the variables it touches, then optimize. GTSAM handles sparsity, ordering, and iSAM2s incremental updates under the hood. \cite{GTSAM_handbook}




%%%%%% START AGAIN FROM HERE!!!! ----------------------------------------------------------------

\paragraph{Batch optimization and linear algebra under the hood.}
Given a nonlinear graph and an initial \texttt{Values}, GTSAM linearizes each factor at the current estimate and stacks the resulting Jacobian blocks into a sparse least-squares system. It then solves with stable square-root methods (QR/Cholesky) or, for very large graphs, iterative methods such as PCG with problem-specific preconditioners. Variable \emph{ordering} is crucial: heuristics like COLAMD, constrained COLAMD, or nested dissection pick elimination orders that reduce fill-in and keep the square-root factor sparse. These are the same ideas you saw in iSAM/iSAM2, exposed as concrete solver choices you can tune. \cite{GTSAM_handbook,iSAM2_paper}

\paragraph{iSAM2 and the Bayes tree in GTSAM.}
For online operation GTSAM implements iSAM2, which maintains a \emph{Bayes tree} instead of a monolithic \(R\) matrix. New factors typically touch only a few variables; iSAM2 identifies just the affected cliques, turns that small region back into a local factor graph, re-eliminates it, and reattaches the untouched “orphan” subtrees at the proper separators. Relinearization is \emph{fluid} and local: mark a variable for refresh only if its change exceeds a (per-state) threshold; propagate solves down the tree only if a parent’s update is large enough. Ordering is maintained incrementally with a \emph{constrained} COLAMD that keeps the newest poses near the root so future updates remain local. All of this is exposed through the \texttt{ISAM2} API: you add factors and initial values, call \texttt{isam.update(\dots)}, and query the current estimate and (local) covariances, with no global refactorization spikes. \cite{iSAM2_paper,Bayes_tree_for_SLAM_paper}

\paragraph{What using GTSAM looks like.}
A typical workflow is straightforward. Start a \texttt{NonlinearFactorGraph} and \texttt{Values} with initial guesses. As measurements arrive, add the corresponding factors (odometry, landmark/vision, loop closures) and insert any new variables. For batch solves, call a \texttt{LevenbergMarquardtOptimizer} or \texttt{GaussNewtonOptimizer} and retrieve the optimized \texttt{Values}. For incremental solves, keep an \texttt{ISAM2} instance alive; on each timestep call \texttt{update(newFactors, newValues)} and read back \texttt{isam.calculateEstimate()}. When you need uncertainties for data association or gating, query marginal covariances from the factorization without forming a dense inverse. You can freely mix 2D/3D poses (\texttt{Pose2}, \texttt{Pose3}), points (\texttt{Point2}/\texttt{Point3}), calibration parameters, and robust noise models, all within the same factor-graph abstraction. \cite{GTSAM_handbook}

\paragraph{Educational and practical.}
GTSAM’s API is intentionally close to the equations: factors are small and typed, values are manifold-aware, and solvers surface sparsity and ordering decisions. The library ships with many examples and bindings (MATLAB/Python) for quick prototyping and visualization. While its emphasis is clarity and research-friendliness (not every kernel is hand-tuned for a specific platform), the algorithms are the ones you have studied—square-root methods, variable elimination, Bayes trees, iSAM2—and they have powered real robotics and vision systems in labs and products. For most applications GTSAM is “more than good enough” as a dependable back-end you can read, extend, and trust. \cite{GTSAM_handbook,iSAM2_paper,Bayes_tree_for_SLAM_paper}

\paragraph{What GTSAM does \emph{not} do (and how to fill the gap).}
GTSAM is a back-end optimizer; it does not detect features, track them, perform loop detection, or decide data association for you. Those front-end tasks live outside and feed GTSAM through factors. Robustness to outliers (bad matches, spurious loops, moving objects) is handled by choosing robust loss functions or switchable/graduated penalties at the factor level. Over very long runs, mitigate graph growth (the “Eiffel Tower” effect) with keyframing and sparsification so cliques stay small and updates remain local. For far-away covariance queries expect higher cost because more of the tree is touched; use approximate/local covariances for routine gating and save exact queries for critical decisions. These are standard, explicit trade-offs that GTSAM exposes rather than hides. \cite{iSAM2_paper,Bayes_tree_for_SLAM_paper}

\paragraph{Takeaway.}
GTSAM provides a principled, factor-graph centric way to model estimation problems and couples it with high-quality batch and incremental solvers built on iSAM2 and the Bayes tree. It preserves the numerical strengths of square-root factorization while delivering the locality you need for real-time operation. If the earlier sections explained why Bayes trees and local elimination matter, GTSAM is the practical, well-documented implementation that lets you apply those ideas immediately in your own SLAM systems. \cite{GTSAM_handbook,iSAM2_paper,Bayes_tree_for_SLAM_paper}
