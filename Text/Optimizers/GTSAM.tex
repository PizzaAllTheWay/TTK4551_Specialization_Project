\subsection{GTSAM}
\todo[inline]{NOTE: THIS CHAPTER NEEDS TO BE MOVED OUT! It needs to become its own whole chapter at the very end after Global Map and trajectory, because GTSAM has a lot more to offer, like reintegration schemes and IMu and Visual odometry with is a lot more that just Optimizer itself, and there I MUST refrece the following papaers and work done by teh amazing people here as GTSAM is a big reaserch project and university and licensce blababla....}
\todo[inline]{GTSAM includes a state of the art IMU handling scheme based on (https://github.com/borglab/gtsam)}
\todo[inline]{Todd Lupton and Salah Sukkarieh, "Visual-Inertial-Aided Navigation for High-Dynamic Motion in Built Environments Without Initial Conditions", TRO, 28(1):61-76, 2012.}
\todo[inline]{Our implementation improves on this using integration on the manifold, as detailed in}
\todo[inline]{Luca Carlone, Zsolt Kira, Chris Beall, Vadim Indelman, and Frank Dellaert, "Eliminating conditionally independent sets in factor graphs: a unifying perspective based on smart factors", Int. Conf. on Robotics and Automation (ICRA), 2014.}
\todo[inline]{Christian Forster, Luca Carlone, Frank Dellaert, and Davide Scaramuzza, "IMU Preintegration on Manifold for Efficient Visual-Inertial Maximum-a-Posteriori Estimation", Robotics: Science and Systems (RSS), 2015.
If you are using the factor in academic work, please cite the publications above.}
Georgia Tech Smoothing and Mapping, GTSAM, is a BSD licensed C++ library for modeling estimation problems as factor graphs and solving them efficiently with batch optimizers and with incremental iSAM2 and Bayes tree methods. The core idea is to represent knowledge as a product of small factors, each involving only a few variables, and then exploit sparsity and variable elimination to compute fast and stable MAP estimates. In practice this gives a single, uniform toolkit for SLAM in 2D and 3D, visual odometry/SLAM, structure from motion, calibration, and related inference tasks, all built on the same mathematical foundation described in the iSAM2 and Bayes tree papers \cite{GTSAM_handbook,iSAM2_paper,Bayes_tree_for_SLAM_paper}.

\paragraph{Design philosophy: graph first, values separate:}
GTSAM cleanly separates the \textit{``model''} from the \textit{``state''}. A \texttt{NonlinearFactorGraph} stores factors (priors, odometry, landmark/vision measurements, loop closures), while a \texttt{Values} container holds one current assignment to the unknowns (poses, landmarks, intrinsics, etc...). The estimate can be changed without touching the graph, and factors can be added or removed without invalidating unrelated variables. This mirrors the math formulation $f(\Theta)=\prod_i f_i(\Theta_i)$. The graph captures structure and sparsity. A particular $\Theta$ provides an assignment that can be evaluated or optimized. \cite{GTSAM_handbook}

\paragraph{Core building blocks:}
Variables use compact \textit{``keys''} (eks: \texttt{Symbol('x',i)} for pose $x_i$, \texttt{Symbol('l',j)} for landmark $l_j$). You build the problem by adding small, typed \textit{``factors''}, each encoding one piece of sensor information plus its noise model. For example \texttt{PriorFactor<Pose2>}, \texttt{BetweenFactor<Pose2>}, \texttt{BearingRangeFactor2D}, camera factors like \texttt{GenericProjectionFactor<Cal3\_S2>}, and many others. Noise models are explicit and first class (\texttt{noiseModel::Isotropic}, \texttt{noiseModel::Diagonal}, robust M-estimators), so units and weighting are clear and consistent. There are different ways to represent rotations in 3D space, however GTSAM has defined poses in Lie groups, this is a mathematical way to describe 3D space including rotation in an easy and intuitive to handle way. Because poses live on curved rotation/pose spaces (SE(2)/SE(3)), GTSAM updates them using \textit{``local coordinates''} and a \textit{``retraction''} operator. GTSAM computes a small 3D/6D increment in a flat tangent space and then maps it back to a valid pose, avoiding angle wrap around and keeping rotations proper. In practice, Gauss-Newton/Levenberg-Marquardt linearization methods ``just works'' with orientations, no ad hoc hacks needed. The end result is that writing SLAM code feels like drawing the factor graph, add one factor per measurement between the variables it touches, then optimize. GTSAM handles sparsity, ordering, and iSAM2s incremental updates under the hood. \cite{GTSAM_handbook}

\paragraph{Batch optimization and linear algebra under the hood:}
In GTSAM, SLAM is posed as a \texttt{NonlinearFactorGraph} with initial \texttt{Values}. At each optimizer step, GTSAM linearize the factors at the current estimate to obtain a linear system. Rather than forming one huge matrix to represent this linearized system, GTSAM solves this linear step by \emph{variable elimination}. Choose an order, combine the factors that touch the next variable, eliminate it, and keep going. The result can be viewed as a Bayes net, grouping by shared separator variables yields a Bayes tree, through which the solution is recovered by simple back substitution. For online use, \texttt{iSAM2} keeps that Bayes tree and, when new measurements arrive or some states need re-linearization, it rebuilds only the small subtree that is affected and reuses the rest unchanged. Speed and memory depend on the elimination order, so GTSAM provides practical heuristics (eks: COLAMD and constrained COLAMD) that reduce fill in and keep the newest poses near the root so updates stay local. In short, add small typed factors, GTSAM handles linearization and sparse elimination, and with \texttt{iSAM2} it updates only where needed. \cite{GTSAM_handbook}

\paragraph{iSAM2 and the Bayes tree in GTSAM:}
For real-time use, GTSAM iSAM2 keeps a Bayes tree data structure, instead of one giant monolithic $R$ matrix. When a new measurement arrives, it usually touches only a few variables, so iSAM2 edits just that small part of the tree, it pulls out the affected piece, re-solves that tiny subproblem, and snaps it back in place while leaving the rest untouched. Re-linearization is local and on-demand, only refreshing variables if it moves past a small (per-state) threshold, and only push the solve down the tree if a parent changed a lot. The variable order is maintained incrementally (via constrained COLAMD heuristics) so the newest poses stay near the root, which keeps future updates local and fast. In code simply add factors and initial guesses, call \texttt{isam.update(\dots)}, and read out the current estimate (and local covariances) without the big compute spikes of global re-factorization. \cite{GTSAM_handbook}

\paragraph{What using GTSAM looks like:}
Create a \texttt{NonlinearFactorGraph} and a \texttt{Values} with first initial guesses. As new sensor data arrives, add the right factors (odometry, landmark/vision, loop closures) and insert any new variables. For a batch solve, run \texttt{GaussNewtonOptimizer} or \texttt{LevenbergMarquardtOptimizer} and read the improved \texttt{Values}. For real-time use, keep an \texttt{ISAM2} object, call \texttt{update(newFactors, newValues)} each step, then get the current best estimate with \texttt{isam.calculateEstimate()}.When uncertainty is required for data association or validation, compute marginal covariances only for the variables that matter, no giant matrix inverse needed. \texttt{Pose2}/\texttt{Pose3}, \texttt{Point2}/\texttt{Point3}, camera calibration, and robust noise models can be mixed in the same graph, and all follow the same pattern. \cite{GTSAM_handbook}

\paragraph{Educational and practical:}
GTSAM is designed to match the mathematics in the iSAM and iSAM2 papers \cite{iSAM_paper,iSAM2_paper}. Each measurement becomes a small, typed \textit{``factor''}. Current guesses live in a \texttt{Values} container that understands poses and rotations, and the solvers make sparsity and variable ordering visible. GTSAM provides clear examples and MATLAB and Python bindings, so ideas can be tested quickly and results plotted with minimal setup. The focus is clarity and research, not necessarily efficiency. There are a lot of abstractions, and being pedagogical and educational comes before optimizations to the code and specific hardware for CPU and GPU maximum performance. The core methods from the iSAM and iSAM2 papers \cite{iSAM_paper,iSAM2_paper} are exactly the same here behind the algorithms, square-root solving, variable elimination, Bayes trees, and iSAM2. GTSAM has powered real robots and vision systems, so for most projects GTSAM is ``good enough'' as a reliable back end that can be read, extended, and trusted. \cite{GTSAM_handbook}

\paragraph{What GTSAM does NOT do (and how to fill the gap):}
GTSAM is a back-end optimizer, it does not detect features, track them, perform loop detection, or decide data association for you. Those front-end tasks live outside and feed GTSAM through factors. Robustness to outliers (bad matches, spurious loops, moving objects) is handled by choosing robust loss functions or switchable/graduated penalties at the factor level. Over very long runs, mitigate graph growth (the ``Eiffel Tower'' effect) with keyframing and sparsification so cliques stay small and updates remain local. For distant covariance queries, expect higher cost because more of the tree is touched. Use approximate or local covariances for routine gating, and reserve exact queries for critical decisions. With clever design these gaps can be resolved. \cite{GTSAM_handbook}

\paragraph{Takeaway:}
GTSAM provides a principled, factor graph centric way to model estimation problems and couples it with high quality batch and incremental solvers built on iSAM2 and the Bayes tree. It preserves the numerical strengths of square root factorization while delivering the locality needed for real-time operation. GTSAM provides a practical implementation of iSAM2 ideas for direct use in SLAM systems. \cite{GTSAM_handbook}


