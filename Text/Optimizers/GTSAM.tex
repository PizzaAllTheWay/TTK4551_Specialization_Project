\subsection{GTSAM}
Georgia Tech Smoothing And Mapping (\gls{GTSAM}) is a BSD-licensed C++ library that lets us model estimation problems as factor graphs and solve them efficiently with both batch optimizers and the incremental \gls{iSAM}2/Bayes tree methods. The guiding idea is to express what we know as a product of small factors, each touching only a few variables, and then exploit sparsity and variable elimination to compute fast, stable \gls{MAP} estimates. In practice this gives a single, uniform toolkit for \gls{SLAM} (2D/3D), visual odometry/\gls{SLAM}, structure from motion, calibration, and related inference tasks, all built on the same mathematical foundation described in the \gls{iSAM}2 and Bayes tree papers \cite{GTSAM_handbook,iSAM2_paper,Bayes_tree_for_SLAM_paper}.

\paragraph{Design philosophy: graph first, values separate:}
\gls{GTSAM} cleanly separates the \textit{``model''} from the \textit{``state''}. A \texttt{NonlinearFactorGraph} stores our factors (priors, odometry, landmark/vision measurements, loop closures), while a \texttt{Values} container holds one current assignment to the unknowns (poses, landmarks, intrinsics, etc...). We can change the estimate without touching the graph, and we can add/remove factors without invalidating unrelated variables. This mirrors the math $f(\Theta)=\prod_i f_i(\Theta_i)$. The graph captures structure and sparsity. A particular $\Theta$ lets us evaluate or optimize. \cite{GTSAM_handbook}

\paragraph{Core building blocks:}
Variables use compact \textit{``keys''} (eks: \texttt{Symbol('x',i)} for pose $x_i$, \texttt{Symbol('l',j)} for landmark $l_j$). You build the problem by adding small, typed \textit{``factors''}, each encoding one piece of sensor information plus its noise model. For example \texttt{PriorFactor<Pose2>}, \texttt{BetweenFactor<Pose2>}, \texttt{BearingRangeFactor2D}, camera factors like \texttt{GenericProjectionFactor<Cal3\_S2>}, and many others. Noise models are explicit and first class (\texttt{noiseModel::Isotropic}, \texttt{noiseModel::Diagonal}, robust M-estimators), so units and weighting are clear and consistent. There are different ways to represent rotations in 3D space, however \gls{GTSAM} has defined poses in Lie groups, this is a mathematical way to describe 3D space including rotation in an easy and intuitive to handle way. Because poses live on curved rotation/pose spaces (SE(2)/SE(3)), \gls{GTSAM} updates them using \textit{``local coordinates''} and a \textit{``retraction''} operator. \gls{GTSAM} computes a small 3D/6D increment in a flat tangent space and then maps it back to a valid pose, avoiding angle wrap around and keeping rotations proper. In practice, Gauss-Newton/Levenberg-Marquardt linearization methods ``just works'' with orientations, no ad hoc hacks needed. The end result is that writing \gls{SLAM} code feels like drawing the factor graph, add one factor per measurement between the variables it touches, then optimize. \gls{GTSAM} handles sparsity, ordering, and iSAM2s incremental updates under the hood. \cite{GTSAM_handbook}

\paragraph{Batch optimization and linear algebra under the hood:}
In \gls{GTSAM} we pose \gls{SLAM} as a \texttt{NonlinearFactorGraph} plus an initial \texttt{Values}. At each optimizer step \gls{GTSAM} linearize the factors at the current estimate to get a linear system witch we can solve for. Rather than forming one huge matrix to represent this linearized system, \gls{GTSAM} solves this linear step by \emph{variable elimination}. Choose an order, combine the factors that touch the next variable, eliminate it, and keep going. The result can be viewed as a Bayes net, grouping by shared separator variables yields a Bayes tree, through which the solution is recovered by simple back substitution. For online use, \texttt{iSAM2} keeps that Bayes tree and, when new measurements arrive or some states need re-linearization, it rebuilds only the small subtree that is affected and reuses the rest unchanged. Speed and memory depend on the elimination order, so \gls{GTSAM} provides practical heuristics (eks: COLAMD and constrained COLAMD) that reduce fill in and keep the newest poses near the root so updates stay local. In short, we add small typed factors, \gls{GTSAM} handles linearization and sparse elimination, and with \texttt{iSAM2} it updates only where needed. \cite{GTSAM_handbook}

\paragraph{iSAM2 and the Bayes tree in GTSAM:}
For real-time use, \gls{GTSAM} \gls{iSAM}2 keeps a Bayes tree data structure, instead of one giant monolithic $R$ matrix. When a new measurement arrives, it usually touches only a few variables, so \gls{iSAM}2 edits just that small part of the tree, it pulls out the affected piece, re-solves that tiny subproblem, and snaps it back in place while leaving the rest untouched. Re-linearization is local and on-demand, only refreshing variables if it moves past a small (per-state) threshold, and only push the solve down the tree if a parent changed a lot. The variable order is maintained incrementally (via constrained COLAMD heuristics) so the newest poses stay near the root, which keeps future updates local and fast. In code we simply add factors and initial guesses, call \texttt{isam.update(\dots)}, and read out the current estimate (and local covariances) without the big compute spikes of global re-factorization. \cite{GTSAM_handbook}

\paragraph{What using GTSAM looks like:}
Create a \texttt{NonlinearFactorGraph} and a \texttt{Values} with our first guesses. As new sensor data arrives, add the right factors (odometry, landmark/vision, loop closures) and insert any new variables. For a batch solve, run \texttt{GaussNewtonOptimizer} or \texttt{LevenbergMarquardtOptimizer} and read the improved \texttt{Values}. For real-time use, we keep an \texttt{ISAM2} object, call \texttt{update(newFactors, newValues)} each step, then get the current best estimate with \texttt{isam.calculateEstimate()}. When we need uncertainty for data association or checks, ask for marginal covariances of just the variables we care about, no giant matrix inverses needed. We can freely mix \texttt{Pose2}/\texttt{Pose3}, \texttt{Point2}/\texttt{Point3}, camera calibration, and robust noise models in the same graph, it all fits the same pattern. \cite{GTSAM_handbook}

\paragraph{Educational and practical:}
\gls{GTSAM} is built to feel like the math we see in the \gls{iSAM} and \gls{iSAM}2 papers \cite{iSAM_paper,iSAM2_paper}. Each measurement becomes a small, typed \textit{``factor''}. Our current guesses live in a \texttt{Values} container that understands poses and rotations, and the solvers make sparsity and variable ordering visible. \gls{GTSAM} comes with clear examples and MATLAB/Python bindings, so we can try ideas quickly and plot results without much setup. The focus is clarity and research, not necessarily efficiency. There are a lot of abstractions, and being pedagogical and educational comes before optimizations to the code and specific hardware for CPU and GPU maximum performance. The core methods from the \gls{iSAM} and \gls{iSAM}2 papers \cite{iSAM_paper,iSAM2_paper} are exactly the same here behind the algorithms, square-root solving, variable elimination, Bayes trees, and \gls{iSAM}2. \gls{GTSAM} has powered real robots and vision systems, so for most projects \gls{GTSAM} is ``good enough'' as a reliable back end we can read, extend, and trust. \cite{GTSAM_handbook}

\paragraph{What GTSAM does NOT do (and how to fill the gap):}
\gls{GTSAM} is a back-end optimizer, it does not detect features, track them, perform loop detection, or decide data association for you. Those front-end tasks live outside and feed \gls{GTSAM} through factors. Robustness to outliers (bad matches, spurious loops, moving objects) is handled by choosing robust loss functions or switchable/graduated penalties at the factor level. Over very long runs, mitigate graph growth (the ``Eiffel Tower'' effect) with keyframing and sparsification so cliques stay small and updates remain local. For far away covariance queries, we must expect higher cost because more of the tree is touched, use of approximate/local covariances for routine gating and saving exact queries for critical decisions is the way to go. With clever design these gaps can be resolved. \cite{GTSAM_handbook}

\paragraph{Takeaway:}
\gls{GTSAM} provides a principled, factor graph centric way to model estimation problems and couples it with high quality batch and incremental solvers built on \gls{iSAM}2 and the Bayes tree. It preserves the numerical strengths of square root factorization while delivering the locality we need for real-time operation. \gls{GTSAM} is the practical implementation that lets us apply \gls{iSAM}2 ideas immediately in our own \gls{SLAM} systems. \cite{GTSAM_handbook}

