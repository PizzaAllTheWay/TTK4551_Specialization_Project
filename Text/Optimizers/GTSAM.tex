\subsection{GTSAM}
Georgia Tech Smoothing And Mapping (GTSAM) is a BSD-licensed C++ library that lets us model estimation problems as factor graphs and solve them efficiently with both batch optimizers and the incremental iSAM2/Bayes tree methods. The guiding idea is to express what we know as a product of small factors, each touching only a few variables, and then exploit sparsity and variable elimination to compute fast, stable MAP estimates. In practice this gives a single, uniform toolkit for SLAM (2D/3D), visual odometry/SLAM, structure from motion, calibration, and related inference tasks, all built on the same mathematical foundation described in the iSAM2 and Bayes tree papers\cite{GTSAM_handbook,iSAM2_paper,Bayes_tree_for_SLAM_paper}.

\paragraph{Design philosophy: graph first, values separate.}
GTSAM cleanly separates the \emph{model} from the \emph{state}. A \texttt{NonlinearFactorGraph} stores your factors (priors, odometry, landmark/vision measurements, loop closures), while a \texttt{Values} container holds one current assignment to the unknowns (poses, landmarks, intrinsics, etc.). You can change the estimate without touching the graph, and you can add/remove factors without invalidating unrelated variables. This mirrors the math \(f(\Theta)=\prod_i f_i(\Theta_i)\): the graph captures structure and sparsity; a particular \(\Theta\) lets you evaluate or optimize. \cite{GTSAM_handbook}

\paragraph{Core building blocks.}
Variables are referenced by compact \emph{keys} (e.g., \texttt{Symbol('x',i)} for pose \(x_i\), \texttt{Symbol('l',j)} for landmark \(l_j\)). Factors are small typed objects that encode a likelihood with an explicit noise model, e.g., \texttt{PriorFactor<Pose2>}, \texttt{BetweenFactor<Pose2>}, \texttt{BearingRangeFactor2D}, or vision factors such as \texttt{GenericProjectionFactor<Cal3\_S2>}. Noise models are first-class (\texttt{noiseModel::Isotropic}, \texttt{Diagonal}, robust losses), so units and weighting are explicit. Because poses live on Lie groups (SE(2)/SE(3)), GTSAM uses manifold-aware “retraction” and local coordinates, making Gauss–Newton/LM behave properly with rotations and avoiding ad-hoc angle hacks. The end result is that writing a SLAM problem in code feels like drawing its factor graph: you add one small factor per piece of information. \cite{GTSAM_handbook}

\paragraph{Batch optimization and linear algebra under the hood.}
Given a nonlinear graph and an initial \texttt{Values}, GTSAM linearizes each factor at the current estimate and stacks the resulting Jacobian blocks into a sparse least-squares system. It then solves with stable square-root methods (QR/Cholesky) or, for very large graphs, iterative methods such as PCG with problem-specific preconditioners. Variable \emph{ordering} is crucial: heuristics like COLAMD, constrained COLAMD, or nested dissection pick elimination orders that reduce fill-in and keep the square-root factor sparse. These are the same ideas you saw in iSAM/iSAM2, exposed as concrete solver choices you can tune. \cite{GTSAM_handbook,iSAM2_paper}

\paragraph{iSAM2 and the Bayes tree in GTSAM.}
For online operation GTSAM implements iSAM2, which maintains a \emph{Bayes tree} instead of a monolithic \(R\) matrix. New factors typically touch only a few variables; iSAM2 identifies just the affected cliques, turns that small region back into a local factor graph, re-eliminates it, and reattaches the untouched “orphan” subtrees at the proper separators. Relinearization is \emph{fluid} and local: mark a variable for refresh only if its change exceeds a (per-state) threshold; propagate solves down the tree only if a parent’s update is large enough. Ordering is maintained incrementally with a \emph{constrained} COLAMD that keeps the newest poses near the root so future updates remain local. All of this is exposed through the \texttt{ISAM2} API: you add factors and initial values, call \texttt{isam.update(\dots)}, and query the current estimate and (local) covariances, with no global refactorization spikes. \cite{iSAM2_paper,Bayes_tree_for_SLAM_paper}

\paragraph{What using GTSAM looks like.}
A typical workflow is straightforward. Start a \texttt{NonlinearFactorGraph} and \texttt{Values} with initial guesses. As measurements arrive, add the corresponding factors (odometry, landmark/vision, loop closures) and insert any new variables. For batch solves, call a \texttt{LevenbergMarquardtOptimizer} or \texttt{GaussNewtonOptimizer} and retrieve the optimized \texttt{Values}. For incremental solves, keep an \texttt{ISAM2} instance alive; on each timestep call \texttt{update(newFactors, newValues)} and read back \texttt{isam.calculateEstimate()}. When you need uncertainties for data association or gating, query marginal covariances from the factorization without forming a dense inverse. You can freely mix 2D/3D poses (\texttt{Pose2}, \texttt{Pose3}), points (\texttt{Point2}/\texttt{Point3}), calibration parameters, and robust noise models, all within the same factor-graph abstraction. \cite{GTSAM_handbook}

\paragraph{Educational and practical.}
GTSAM’s API is intentionally close to the equations: factors are small and typed, values are manifold-aware, and solvers surface sparsity and ordering decisions. The library ships with many examples and bindings (MATLAB/Python) for quick prototyping and visualization. While its emphasis is clarity and research-friendliness (not every kernel is hand-tuned for a specific platform), the algorithms are the ones you have studied—square-root methods, variable elimination, Bayes trees, iSAM2—and they have powered real robotics and vision systems in labs and products. For most applications GTSAM is “more than good enough” as a dependable back-end you can read, extend, and trust. \cite{GTSAM_handbook,iSAM2_paper,Bayes_tree_for_SLAM_paper}

\paragraph{What GTSAM does \emph{not} do (and how to fill the gap).}
GTSAM is a back-end optimizer; it does not detect features, track them, perform loop detection, or decide data association for you. Those front-end tasks live outside and feed GTSAM through factors. Robustness to outliers (bad matches, spurious loops, moving objects) is handled by choosing robust loss functions or switchable/graduated penalties at the factor level. Over very long runs, mitigate graph growth (the “Eiffel Tower” effect) with keyframing and sparsification so cliques stay small and updates remain local. For far-away covariance queries expect higher cost because more of the tree is touched; use approximate/local covariances for routine gating and save exact queries for critical decisions. These are standard, explicit trade-offs that GTSAM exposes rather than hides. \cite{iSAM2_paper,Bayes_tree_for_SLAM_paper}

\paragraph{Takeaway.}
GTSAM provides a principled, factor-graph centric way to model estimation problems and couples it with high-quality batch and incremental solvers built on iSAM2 and the Bayes tree. It preserves the numerical strengths of square-root factorization while delivering the locality you need for real-time operation. If the earlier sections explained why Bayes trees and local elimination matter, GTSAM is the practical, well-documented implementation that lets you apply those ideas immediately in your own SLAM systems. \cite{GTSAM_handbook,iSAM2_paper,Bayes_tree_for_SLAM_paper}
